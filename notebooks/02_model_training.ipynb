{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Object Detection - Model Training\n",
    "\n",
    "This notebook demonstrates how to train RetinaNet and Deformable DETR models for traffic object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "from datasets import TrafficDataset, get_transforms\n",
    "from models import create_model\n",
    "from train import Trainer\n",
    "from utils.config import load_config, create_default_config\n",
    "from utils.logger import setup_logger\n",
    "from utils.visualization import plot_training_curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = \"../configs/retinanet_config.yaml\"\n",
    "if os.path.exists(config_path):\n",
    "    config = load_config(config_path)\n",
    "else:\n",
    "    print(\"Config file not found, creating default configuration\")\n",
    "    config = create_default_config()\n",
    "\n",
    "# Override for notebook training\n",
    "config.training.epochs = 10  # Shorter training for demo\n",
    "config.dataset.batch_size = 4  # Smaller batch size\n",
    "config.dataset.image_size = 512\n",
    "\n",
    "print(f\"Model: {config.model.name}\")\n",
    "print(f\"Epochs: {config.training.epochs}\")\n",
    "print(f\"Batch size: {config.dataset.batch_size}\")\n",
    "print(f\"Learning rate: {config.training.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "dataset_root = \"../data/traffic\"\n",
    "images_dir = os.path.join(dataset_root, \"images\")\n",
    "train_annotations = os.path.join(dataset_root, \"train_annotations.json\")\n",
    "val_annotations = os.path.join(dataset_root, \"val_annotations.json\")\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(train_annotations):\n",
    "    print(\"Warning: Training annotations not found!\")\n",
    "    print(f\"Expected: {train_annotations}\")\n",
    "    print(\"Please run the data download script first.\")\n",
    "else:\n",
    "    print(\"Dataset found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets and data loaders\n",
    "def create_data_loaders(config):\n",
    "    # Get transforms\n",
    "    train_transform = get_transforms(\n",
    "        phase=\"train\",\n",
    "        image_size=config.dataset.image_size\n",
    "    )\n",
    "    val_transform = get_transforms(\n",
    "        phase=\"val\",\n",
    "        image_size=config.dataset.image_size\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TrafficDataset(\n",
    "        root=images_dir,\n",
    "        annotation_file=train_annotations,\n",
    "        transform=train_transform,\n",
    "        class_names=config.classes.names\n",
    "    )\n",
    "    \n",
    "    val_dataset = TrafficDataset(\n",
    "        root=images_dir,\n",
    "        annotation_file=val_annotations,\n",
    "        transform=val_transform,\n",
    "        class_names=config.classes.names\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.dataset.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,  # Reduced for notebook\n",
    "        collate_fn=lambda x: tuple(zip(*x))\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config.dataset.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        collate_fn=lambda x: tuple(zip(*x))\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, train_dataset, val_dataset\n",
    "\n",
    "if os.path.exists(train_annotations):\n",
    "    train_loader, val_loader, train_dataset, val_dataset = create_data_loaders(config)\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Number of classes: {len(config.classes.names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = create_model(\n",
    "    model_name=config.model.name,\n",
    "    num_classes=config.model.num_classes,\n",
    "    config=config.model,\n",
    "    pretrained=config.model.pretrained\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: {type(model).__name__}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer\n",
    "if config.training.optimizer.lower() == \"adamw\":\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config.training.learning_rate,\n",
    "        weight_decay=config.training.weight_decay\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config.training.learning_rate,\n",
    "        momentum=0.9,\n",
    "        weight_decay=config.training.weight_decay\n",
    "    )\n",
    "\n",
    "# Create scheduler\n",
    "if config.training.scheduler.lower() == \"cosine\":\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=config.training.epochs\n",
    "    )\n",
    "else:\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=30, gamma=0.1\n",
    "    )\n",
    "\n",
    "print(f\"Optimizer: {type(optimizer).__name__}\")\n",
    "print(f\"Scheduler: {type(scheduler).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "if os.path.exists(train_annotations):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        save_dir=\"../checkpoints/notebook_training\"\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Train the model\n",
    "    history = trainer.train(\n",
    "        epochs=config.training.epochs,\n",
    "        save_every=5\n",
    "    )\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "else:\n",
    "    print(\"Skipping training - dataset not available\")\n",
    "    history = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if history is not None:\n",
    "    # Plot training curves\n",
    "    fig = plot_training_curves(\n",
    "        train_losses=history['train_losses'],\n",
    "        val_losses=history['val_losses'],\n",
    "        learning_rates=history['learning_rates']\n",
    "    )\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(f\"\\nFinal Training Results:\")\n",
    "    print(f\"Final training loss: {history['train_losses'][-1]:.4f}\")\n",
    "    print(f\"Final validation loss: {history['val_losses'][-1]:.4f}\")\n",
    "    print(f\"Best validation loss: {min(history['val_losses']):.4f}\")\n",
    "    print(f\"Final learning rate: {history['learning_rates'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(train_annotations):\n",
    "    # Test inference on a few samples\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a batch from validation set\n",
    "    val_iter = iter(val_loader)\n",
    "    images, targets = next(val_iter)\n",
    "    \n",
    "    # Move to device\n",
    "    images_device = [img.to(device) for img in images]\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model.predict(images_device, score_threshold=0.3)\n",
    "    \n",
    "    # Print results\n",
    "    for i, (pred, target) in enumerate(zip(predictions, targets)):\n",
    "        print(f\"\\nImage {i+1}:\")\n",
    "        print(f\"  Ground truth objects: {len(target['boxes'])}\")\n",
    "        print(f\"  Predicted objects: {len(pred['boxes'])}\")\n",
    "        \n",
    "        if len(pred['boxes']) > 0:\n",
    "            # Show top predictions\n",
    "            top_scores, top_indices = torch.topk(pred['scores'], min(3, len(pred['scores'])))\n",
    "            for j, idx in enumerate(top_indices):\n",
    "                class_id = pred['labels'][idx].item()\n",
    "                score = pred['scores'][idx].item()\n",
    "                class_name = config.classes.names[class_id] if class_id < len(config.classes.names) else f\"Class_{class_id}\"\n",
    "                print(f\"    {j+1}. {class_name}: {score:.3f}\")\n",
    "else:\n",
    "    print(\"Skipping inference test - dataset not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "output_dir = \"../outputs/notebook_training\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(output_dir, \"trained_model.pth\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Save configuration\n",
    "config_save_path = os.path.join(output_dir, \"config.yaml\")\n",
    "from utils.config import save_config\n",
    "save_config(config, config_save_path)\n",
    "print(f\"Configuration saved to: {config_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After training, you can:\n",
    "\n",
    "1. **Evaluate the model**: Use the evaluation notebook to assess model performance\n",
    "2. **Run inference**: Test the model on new images using the inference script\n",
    "3. **Compare models**: Train different architectures and compare their performance\n",
    "4. **Fine-tune**: Adjust hyperparameters and retrain for better results\n",
    "5. **Deploy**: Use the trained model in a production environment\n",
    "\n",
    "For production training, use the command-line scripts with full datasets and longer training times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}